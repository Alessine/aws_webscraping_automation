{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44121a52-c087-4b9a-bdcb-a37b5cf40148",
   "metadata": {},
   "source": [
    "<a id='Q0'></a>\n",
    "<center> <h1> Notebook 1: Developing a Script for Retrieving Data and Saving it in an AWS S3 Bucket</h1> </center>\n",
    "<p style=\"margin-bottom:1cm;\"></p>\n",
    "<center><strong>Angela Niederberger, 2022</strong></center>\n",
    "<p style=\"margin-bottom:1cm;\"></p>\n",
    "\n",
    "<div style=\"background:#EEEDF5;border-top:0.1cm solid #EF475B;border-bottom:0.1cm solid #EF475B;\">\n",
    "    <div style=\"margin-left: 0.5cm;margin-top: 0.5cm;margin-bottom: 0.5cm;color:#303030\">\n",
    "        <p><strong>Goal:</strong> In this notebook I develop a python script to retrieve data on the most recent news items from the SRF website and save it in an AWS S3 bucket.</p>\n",
    "        <strong> Outline:</strong>\n",
    "        <a id='P0' name=\"P0\"></a>\n",
    "        <ol>\n",
    "            <li> <a style=\"color:#303030\" href='#I'>Introduction </a> </li>\n",
    "            <li> <a style=\"color:#303030\" href='#SU'>Set up</a></li>\n",
    "            <li> <a style=\"color:#303030\" href='#P1'>Retrieving the Data</a></li>\n",
    "            <li> <a style=\"color:#303030\" href='#P2'>Saving the Data</a></li>\n",
    "            <li> <a style=\"color:#303030\" href='#P3'>Code Refactor</a></li>\n",
    "            <li> <a style=\"color:#303030\" href='#CL'>Conclusion</a></li>\n",
    "        </ol>\n",
    "        <strong>Keywords:</strong> Webscraping, BeautifulSoup, AWS S3\n",
    "    </div>\n",
    "</div>\n",
    "</nav>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b888b7d-daaa-42f3-bf20-8b34f2032708",
   "metadata": {},
   "source": [
    "<a id='I' name=\"I\"></a>\n",
    "## [Introduction](#P0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48c8fbb-3714-4baf-93b8-ea539f476ded",
   "metadata": {},
   "source": [
    "In this notebook I explore the following steps:\n",
    "- how to retrieve data from a website using BeautifulSoup\n",
    "- performing basic cleaning tasks on this data\n",
    "- saving it to an AWS S3 Bucket\n",
    "\n",
    "Then I rewrite my code to create a Python script that can be automated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e986ed40-d5ad-4d4e-8039-a66346f7f9bd",
   "metadata": {},
   "source": [
    "<a id='SU' name=\"SU\"></a>\n",
    "## [Set up](#P0)\n",
    "\n",
    "I created a virtual environment and installed Python 3.10.4 in it for this project. Below are the specifics on all the packages I used. I've also summarized all of this information in the requirements file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a290be9-7423-49dd-83d5-011aa101841a",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1b2595c-4350-411c-af49-350ca1a196f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving & Wrangling Data\n",
    "import requests  # Version 2.27.1\n",
    "from bs4 import BeautifulSoup  # Version 4.11.1\n",
    "import pandas as pd  # Version 1.4.2\n",
    "from datetime import date  # Built-in Python library\n",
    "\n",
    "# Saving the Data\n",
    "import s3fs\n",
    "\n",
    "# Building the script\n",
    "import logging  # Built-in Python library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0875f827-3f69-45ca-a7b6-f60afe2df07a",
   "metadata": {},
   "source": [
    "### Magic Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfe5d540-623b-4221-81fb-fa7605d01c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de42c31f-c73a-4af6-b9be-2b8f334efb27",
   "metadata": {},
   "source": [
    "<a id='P1'></a>\n",
    "## [Retrieving the Data](#P0)\n",
    "\n",
    "In this first section I use requests and BeautifulSoup to retrieve data on news items from the SRF website. Then I wrangle the data into the desired format with Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe441e86-4914-490c-8edb-e5267bb8c535",
   "metadata": {},
   "source": [
    "### Webscraping\n",
    "\n",
    "Below is a simple webscraping script which serves to extract the publishing time and teaser data of all news articles found on https://www.srf.ch/news/das-neueste on any specific day.\n",
    "\n",
    "Here's an excellent ressource for more information on webscraping: https://realpython.com/beautiful-soup-web-scraper-python/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dfecab4-7524-4d7c-a003-56c22e01e729",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_published</th>\n",
       "      <th>kicker</th>\n",
       "      <th>title</th>\n",
       "      <th>lead</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-06-01T14:33:00+02:00</td>\n",
       "      <td>Krieg in der Ukraine</td>\n",
       "      <td>Zögerliche Zeitenwende: Berlin fremdelt mit se...</td>\n",
       "      <td>Nach langem Zaudern sagt der deutsche Bundeska...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-06-01T14:03:00+02:00</td>\n",
       "      <td>Gesetz gegen Renditesanierung</td>\n",
       "      <td>Zoff um das Basler Mietgesetz</td>\n",
       "      <td>Für Sanierungen gelten in Basel neu strenge Be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-06-01T13:44:00+02:00</td>\n",
       "      <td>Fussball und der Krieg</td>\n",
       "      <td>Als Russen und Ukrainer noch gemeinsam auf dem...</td>\n",
       "      <td>Die Ukraine will an die WM, Russland schaut zu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-06-01T12:55:00+02:00</td>\n",
       "      <td>Kampf gegen Prämienexplosion</td>\n",
       "      <td>Nationalrat will Prämienanstieg mit Kostenziel...</td>\n",
       "      <td>Kosten- und Qualitätsziele sollen Prämien eind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-06-01T12:36:00+02:00</td>\n",
       "      <td>Nachtragskredite wegen Covid</td>\n",
       "      <td>Hitzige Diskussion um 2.1. Milliarden für Kurz...</td>\n",
       "      <td>Wegen eines Bundesgerichtsurteils erhalten Unt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              time_published                         kicker  \\\n",
       "0  2022-06-01T14:33:00+02:00           Krieg in der Ukraine   \n",
       "1  2022-06-01T14:03:00+02:00  Gesetz gegen Renditesanierung   \n",
       "2  2022-06-01T13:44:00+02:00         Fussball und der Krieg   \n",
       "3  2022-06-01T12:55:00+02:00   Kampf gegen Prämienexplosion   \n",
       "4  2022-06-01T12:36:00+02:00   Nachtragskredite wegen Covid   \n",
       "\n",
       "                                               title  \\\n",
       "0  Zögerliche Zeitenwende: Berlin fremdelt mit se...   \n",
       "1                      Zoff um das Basler Mietgesetz   \n",
       "2  Als Russen und Ukrainer noch gemeinsam auf dem...   \n",
       "3  Nationalrat will Prämienanstieg mit Kostenziel...   \n",
       "4  Hitzige Diskussion um 2.1. Milliarden für Kurz...   \n",
       "\n",
       "                                                lead  \n",
       "0  Nach langem Zaudern sagt der deutsche Bundeska...  \n",
       "1  Für Sanierungen gelten in Basel neu strenge Be...  \n",
       "2  Die Ukraine will an die WM, Russland schaut zu...  \n",
       "3  Kosten- und Qualitätsziele sollen Prämien eind...  \n",
       "4  Wegen eines Bundesgerichtsurteils erhalten Unt...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the static page to scrape\n",
    "url = \"https://www.srf.ch/news/das-neueste\"\n",
    "page = requests.get(url)\n",
    "\n",
    "# Then parse it\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "# Get the different elements\n",
    "teaser_lists = soup.find_all(\"div\", class_=\"js-teaser-data\")\n",
    "kickers = soup.find_all(\"span\", class_=\"teaser__kicker-text\")\n",
    "titles = soup.find_all(\"span\", class_=\"teaser__title\")\n",
    "leads = soup.find_all(\"p\", class_=\"teaser__lead\")\n",
    "\n",
    "# Iterate through the elements and extract the relevant information\n",
    "news_snippet_dict = {\n",
    "    \"time_published\": [],\n",
    "    \"kicker\": [],\n",
    "    \"title\": [],\n",
    "    \"lead\": []\n",
    "}\n",
    "\n",
    "for (teaser_list, kicker, title, lead) in zip(teaser_lists, kickers, titles, leads):\n",
    "    news_snippet_dict[\"time_published\"].append(teaser_list.get(\"data-date-published\"))\n",
    "    news_snippet_dict[\"kicker\"].append(kicker.get_text())\n",
    "    news_snippet_dict[\"title\"].append(title.get_text())\n",
    "    news_snippet_dict[\"lead\"].append(lead.get_text())\n",
    "    \n",
    "news_snippets_df = pd.DataFrame(news_snippet_dict)\n",
    "news_snippets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7e85fd-75b9-445b-9fc3-1df02476e5af",
   "metadata": {},
   "source": [
    "### Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "715e89a8-701d-46af-83e0-5fcdc784a75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the publishing time into a timestamp\n",
    "news_snippets_df[\"time_published\"] = pd.to_datetime(news_snippets_df[\"time_published\"])\n",
    "\n",
    "# Select only news from this day\n",
    "news_snippets_df = news_snippets_df[news_snippets_df[\"time_published\"].dt.date==date.today()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c442ba1c-e8be-4ccc-87ed-e0c6a598d88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 20 entries, 0 to 19\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count  Dtype                                \n",
      "---  ------          --------------  -----                                \n",
      " 0   time_published  20 non-null     datetime64[ns, pytz.FixedOffset(120)]\n",
      " 1   kicker          20 non-null     object                               \n",
      " 2   title           20 non-null     object                               \n",
      " 3   lead            20 non-null     object                               \n",
      "dtypes: datetime64[ns, pytz.FixedOffset(120)](1), object(3)\n",
      "memory usage: 800.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "news_snippets_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d090db-9712-40ad-8d7c-70dca34d256c",
   "metadata": {},
   "source": [
    "<a id='P2' name=\"P2\"></a>\n",
    "## [Saving the Data](#P0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed9ebe0-6bc5-4a3e-b537-8d75b42d92c0",
   "metadata": {},
   "source": [
    "### AWS Set-up\n",
    "\n",
    "In order to save data to an S3 Bucket, I first had to complete the following steps:\n",
    "- sign up for an AWS account\n",
    "- with the root user, create an admin user\n",
    "- with the admin user, create an S3 bucket\n",
    "- attach a policy to this bucket, which gives the admin user write access\n",
    "\n",
    "To complete all these steps correctly, I followed this walkthrough provided by AWS: https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-walkthroughs-managing-access-example1.html."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99bf94f-9849-40cb-b62a-c50c173dff01",
   "metadata": {},
   "source": [
    "<div style=\"background:#EEEDF5;border:0.1cm solid #00BAE5;color:#303030\">\n",
    "    <div style=\"margin: 0.2cm 0.2cm 0.2cm 0.2cm\">\n",
    "        <b style=\"color:#00BAE5\">Note:</b>\n",
    "        This is the most crucial part! Once everything is set up correctly in the AWS cloud, it is actually very easy to save files in the bucket.\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fd97df-726f-46a4-857f-46ed272622a1",
   "metadata": {},
   "source": [
    "### Interacting with AWS\n",
    "\n",
    "Now, I can get started with the code below. I am using the S3fs package to interact with the S3 file systems from Python.  For more information, here's the documentation: https://s3fs.readthedocs.io/en/latest/. However, it's not really necessary for saving files. This can be done with Pandas, just like when I'm saving a file to my local drive.\n",
    "\n",
    "To stay organized, I created a `data` folder in my S3 Bucket, so when I write data to the bucket, I need to include this in the file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9257c4a3-5234-49a4-8e72-9b2e970d3ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path for the file\n",
    "s3_folder_path = \"s3://srf-news-snippets/data\"\n",
    "filename = f\"{date.today()}_srf_news_snippets.csv\"\n",
    "\n",
    "# Save the file\n",
    "news_snippets_df.to_csv(f\"{s3_folder_path}/{filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dacdda-9bfb-49f2-b1ef-4cffaa41ae81",
   "metadata": {},
   "source": [
    "<a id='P3' name=\"P3\"></a>\n",
    "## [Code Refactor](#P0)\n",
    "\n",
    "Now I want to combine all of my code into one Python script that can be automated. To do this, I will write some functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ebf382-5063-4d1b-945b-4be69cc9ee73",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bed96e8-24c5-4013-8425-2adf46aa75ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_srf_daily_news(url):\n",
    "    \"\"\"\n",
    "    This function takes in the URL from the SRF news page.\n",
    "    It scrapes this page and returns a dataframe with information on the day's news teasers.\n",
    "    \n",
    "    Required arguments:\n",
    "    - URL: string, the SRF website to be scraped\n",
    "    \"\"\"\n",
    "    page = requests.get(url)\n",
    "\n",
    "    # Then parse it\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "    # Get the different elements\n",
    "    teaser_lists = soup.find_all(\"div\", \n",
    "                                 class_=\"js-teaser-data\")\n",
    "    kickers = soup.find_all(\"span\", class_=\"teaser__kicker-text\")\n",
    "    titles = soup.find_all(\"span\", class_=\"teaser__title\")\n",
    "    leads = soup.find_all(\"p\", class_=\"teaser__lead\")\n",
    "\n",
    "    # Iterate through the elements and extract the relevant information\n",
    "    news_snippet_dict = {\n",
    "        \"time_published\": [],\n",
    "        \"kicker\": [],\n",
    "        \"title\": [],\n",
    "        \"lead\": []\n",
    "    }\n",
    "\n",
    "    for (teaser_list, kicker, title, lead) in zip(teaser_lists, kickers, titles, leads):\n",
    "        news_snippet_dict[\"time_published\"].append(teaser_list.get(\"data-date-published\"))\n",
    "        news_snippet_dict[\"kicker\"].append(kicker.get_text())\n",
    "        news_snippet_dict[\"title\"].append(title.get_text())\n",
    "        news_snippet_dict[\"lead\"].append(lead.get_text())\n",
    "\n",
    "    news_snippets_df = pd.DataFrame(news_snippet_dict)\n",
    "    \n",
    "    # Turn the publishing time into a timestamp\n",
    "    news_snippets_df[\"time_published\"] = pd.to_datetime(news_snippets_df[\"time_published\"])\n",
    "\n",
    "    # Select only news from this day\n",
    "    news_snippets_df = news_snippets_df[news_snippets_df[\"time_published\"].dt.date==\n",
    "                                        date.today()]\n",
    "    \n",
    "    return news_snippets_df\n",
    "  \n",
    "    \n",
    "def main(url, s3_folder_path, filename):\n",
    "    \"\"\"\n",
    "    This function combines the scraping and saving functions.\n",
    "    It logs the progress and prints out info messages.\n",
    "    \"\"\"\n",
    "    # Start logging\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    # Scrape the data\n",
    "    todays_news_df = scrape_srf_daily_news(url=url)\n",
    "    logger.info('Data retrieved')\n",
    "    \n",
    "    # Save it to the bucket\n",
    "    todays_news_df.to_csv(f\"{s3_folder_path}/{filename}\")\n",
    "    logger.info('File saved to bucket')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872d2b6f-18dd-4f57-b14d-930b9ff3f2bb",
   "metadata": {},
   "source": [
    "### Check the Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6afbf921-4948-4e4e-a336-ebe07f3ded97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-01 14:56:23,914 - __main__ - INFO - Data retrieved\n",
      "2022-06-01 14:56:24,251 - botocore.credentials - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2022-06-01 14:56:24,462 - __main__ - INFO - File saved to bucket\n"
     ]
    }
   ],
   "source": [
    "# Define the required arguments\n",
    "srf_news_site = \"https://www.srf.ch/news/das-neueste\"\n",
    "s3_data_folder = \"s3://srf-news-snippets/data\"\n",
    "file = f\"{date.today()}_srf_news_snippets.csv\"\n",
    "\n",
    "# Configure the logging\n",
    "log_fmt = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "logging.basicConfig(level=logging.INFO, format=log_fmt)\n",
    "\n",
    "# Run the main function\n",
    "main(url=srf_news_site, s3_folder_path=s3_data_folder, filename=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0f79d6e-0721-4df3-ad6f-5684af72341f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-01 14:57:02,131 - botocore.credentials - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['srf-news-snippets/data/',\n",
       " 'srf-news-snippets/data/2022-06-01_srf_news_snippets.csv']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the new file was saved\n",
    "fs = s3fs.S3FileSystem(anon=False)\n",
    "fs.ls(s3_data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57561bd5-2390-43be-b47e-6450df034c69",
   "metadata": {},
   "source": [
    "<a id='CL'></a>\n",
    "## [Conclusion](#P0)\n",
    "\n",
    "In this notebook, I developed some code for scraping the SRF News site, store the information in a dataframe and finally save it to a csv file in an AWS S3 bucket. Then I refactored this code into functions, which I can use as a python script. The next step will be to automate the execution of this script in the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66741e1f-51a5-49b7-8012-077607aacbca",
   "metadata": {},
   "source": [
    "<div style=\"border-top:0.1cm solid #EF475B\"></div>\n",
    "    <strong><a href='#Q0'><div style=\"text-align: right\"> <h3>End of this Notebook.</h3></div></a></strong>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_webscraping_automation",
   "language": "python",
   "name": "aws_webscraping_automation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
